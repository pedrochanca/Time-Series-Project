{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Description\n",
    "The file “2014-2018 PM10 LisAvLib” contains a time series of hourly-levels of $PM_{10}$ particles (in micrograms per cubic meter), collected at Avenida da Liberdade monitoring station in Lisbon from 01/01/2014 to 31/12/2018.\n",
    "\n",
    "# Main Goal\n",
    "Fit a **SARIMA-type** model to the time series representing 24-h average levels of $PM_{10}$ particles. \n",
    "\n",
    "# Process\n",
    "\n",
    "1. Libraries;\n",
    "2. Dataset Importation;\n",
    "3. Functions;\n",
    "4. Exploratory Data Analysis:\n",
    "    1. Handling Missing Values;\n",
    "    2. Statistical and Empirical Analysis;\n",
    "    3. ACF and PACF;\n",
    "    4. Decomposing;\n",
    "    5. Identification of the dependence of orders and degree of differencing;\n",
    "    6. Transformations.\n",
    "5. Model Fitting and Diagnostics:\n",
    "    1. Parameter Estimation;\n",
    "    2. Residual diagnostics and model selection.\n",
    "6. Cross-validation;\n",
    "7. Forecast:\n",
    "    1. Forecast the data into the future up to 5 time periods ahead;\n",
    "    2. Calculate 95% prediction intervals for each of the 5 forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xlrd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from pandas import Series\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.graphics.api import qqplot\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "import pmdarima.arima as pm\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, mean_squared_log_error\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "import pylab\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "from numpy import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcdefaults()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 4)\n",
    "\n",
    "font = {'family' : 'monospace',\n",
    "        'size'   : 16}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "matplotlib.style.use('seaborn-whitegrid')\n",
    "\n",
    "tfont = {'family' : 'monospace',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 18}\n",
    "\n",
    "yfont = {'family' : 'serif',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 18}\n",
    "\n",
    "lfont = {'family' : 'serif',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 14}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import dataset\n",
    "\n",
    "- The name of the 2 initial columns was changes to 'Date' and 'PM_10' ir order to facilitate future analysis.\n",
    "\n",
    "- Set the first column, 'Date', as the index of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_excel('2014-2018 PM10 LisAvLib.xlsx', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['Date','PM_10']\n",
    "df=df.set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The functions below will be used along the course of this project. They were created in order to facilitate the user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_mean(ts, n):\n",
    "    out = np.copy(ts)\n",
    "    for i, val in enumerate(ts):\n",
    "        if np.isnan(val):\n",
    "            n_by_2 = np.ceil(n/2)\n",
    "            lower = np.max([0, int(i-n_by_2)])\n",
    "            upper = np.min([len(ts)+1, int(i+n_by_2)])\n",
    "            ts_near = np.concatenate([ts[lower:i], ts[i:upper]])\n",
    "            out[i] = np.nanmean(ts_near)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation(x, n_rep, n_na, n_k):\n",
    "\n",
    "    #Inputs:\n",
    "        #x - 24h dataframe\n",
    "        #n_rep - number of cycles\n",
    "        #n_na - number of missing values on the new test dataframe\n",
    "        #n_k - number of k neighbours\n",
    "        \n",
    "    warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "    \n",
    "    #Creating a dataframe with the first 500 values of the 24-hour average dataset\n",
    "    y=pd.DataFrame(x,columns='PM_10'.split(),index=x.index[:500])\n",
    "    \n",
    "    #Creating 5 different arrays to store the errors for each imputation technique on each cycle\n",
    "    c1=[]\n",
    "    c2=[]\n",
    "    c3=[]\n",
    "    c4=[]\n",
    "    c5=[]\n",
    "\n",
    "    print('Forward Fill | Backward Fill | Linear Interpolation | Cubic Interpolation | k-NN')\n",
    "    for i in range(0, n_rep):\n",
    "        #Create a new column on the dfIMPtest dataframe with the same values as the column PM_10\n",
    "        y = y.assign(PM_10NA=y['PM_10'])\n",
    "        \n",
    "        #Replace the values of 10 random objects from the column 'PM_10' with NA value\n",
    "        change = y.sample(n_na).index\n",
    "        y.loc[change,'PM_10NA'] = np.nan\n",
    "        \n",
    "        #Forward Fill\n",
    "        y['ffill'] = y['PM_10NA'].ffill()\n",
    "        error_ffill = np.round(mean_squared_error(y['PM_10'], y['ffill']), 2)\n",
    "        c1.append(error_ffill)\n",
    "        \n",
    "        #Backward Fill\n",
    "        y['bfill'] = y['PM_10NA'].bfill()\n",
    "        error_bfill = np.round(mean_squared_error(y['PM_10'], y['bfill']), 2)\n",
    "        c2.append(error_bfill)\n",
    "        \n",
    "        #Creating a new column of y dataframe which will be used in Linear and Cubic Interpolation\n",
    "        y['rownum'] = np.arange(y.shape[0])\n",
    "        y_nona = y.dropna(subset = ['PM_10NA'])\n",
    "\n",
    "        #Linear Interpolation\n",
    "        f = interp1d(y_nona['rownum'], y_nona['PM_10NA'])\n",
    "        y['linear_fill'] = f(y['rownum'])\n",
    "        error_lininter = np.round(mean_squared_error(y['PM_10'], y['linear_fill']), 2)\n",
    "        c3.append(error_lininter)\n",
    "        \n",
    "        #Cubic Interpolation\n",
    "        f2 = interp1d(y_nona['rownum'], y_nona['PM_10NA'],kind='cubic')\n",
    "        y['cubic_fill'] = f2(y['rownum'])\n",
    "        error_cubicinter = np.round(mean_squared_error(y['PM_10'], y['cubic_fill']), 2)\n",
    "        c4.append(error_cubicinter)\n",
    "        \n",
    "        #k-NN\n",
    "        y['knn_mean'] = knn_mean(y.PM_10NA.values, n_k)\n",
    "        error_knn = np.round(mean_squared_error(y['PM_10'], y['knn_mean']), 2)\n",
    "        c5.append(error_knn)\n",
    "        \n",
    "        print('{} - {} | {} | {} | {} | {} '.format(i+1, c1[i],c2[i],c3[i],c4[i],c5[i]))\n",
    "        #print('Ite:',i)\n",
    "    \n",
    "    #Average error values \n",
    "    avg_forward=np.mean(c1)\n",
    "    avg_backward=np.mean(c2)\n",
    "    avg_lininter=np.mean(c3)\n",
    "    avg_cubicinter=np.mean(c4)\n",
    "    avg_knn=np.mean(c5)\n",
    "    \n",
    "    print('Forward Fill Average:',avg_forward)\n",
    "    print('Backward Fill Average:',avg_backward)\n",
    "    print('Linear Interpolation Average:', avg_lininter)\n",
    "    print('Cubic Interpolation Average:', avg_cubicinter)\n",
    "    print('k-NN Average:', avg_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_analysis(df,nan):\n",
    "    print('----Descriptive-Analysis----')\n",
    "    print('Description: ',df.describe())\n",
    "    print('----------------------------')\n",
    "    print('Mean : ',df.mean())\n",
    "    print('Variance : ',df.var())\n",
    "    print('Skewness : ',df.skew())\n",
    "    print('Kurtosis : ',df.kurt())\n",
    "    print('--------------@-------------')\n",
    "    \n",
    "    df.plot(figsize=(12, 4))\n",
    "    if nan==0:\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "        df.plot.kde(figsize=(12, 4),ax=axes[0])\n",
    "        qqplot(df, line='q', fit=True, ax=axes[1])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ARMA_search(y,pr,qr):\n",
    "\n",
    "    #Inputs:\n",
    "        #y - Dataframe\n",
    "        #p_r - max range of p\n",
    "        #q_r - max range of q\n",
    "        \n",
    "    warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "\n",
    "    # Define the p and q parameters to take any value between 0 and p_r/q_r, respectively.\n",
    "    p = range(0, pr)\n",
    "    q = range(0, qr)\n",
    "    \n",
    "    # Generate all different combinations of p and q\n",
    "    pq = list(itertools.product(p,q))\n",
    "\n",
    "    c1=[]\n",
    "    c2=[]\n",
    "    i=0\n",
    "    \n",
    "    for k in pq:\n",
    "        try:\n",
    "            model=sm.tsa.ARMA(y,order=k)\n",
    "            results = model.fit()\n",
    "                \n",
    "            c1.append(results.aic)\n",
    "            c2.append(results.bic)\n",
    "            print('{}-ARMA{} - AIC:{} | BIC:{}'.format(i,k, results.aic, results.bic))\n",
    "            i=i+1\n",
    "        except:\n",
    "            continue\n",
    "  \n",
    "    index_min1 = np.argmin(c1)\n",
    "    index_min2 = np.argmin(c2)\n",
    "\n",
    "    print('Valor minimo de AIC:',np.min(c1))\n",
    "    print('Combinacao de parametros:', index_min1)\n",
    "    \n",
    "    print('Valor minimo de BIC:',np.min(c2))\n",
    "    print('Combinacao de parametros:', index_min2)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_results(y,train,pr,qr):\n",
    "    #Inputs:\n",
    "        #y - Dataframe\n",
    "        #p_r - max range of p\n",
    "        #q_r - max range of q\n",
    "        \n",
    "    warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "\n",
    "    start_index = y.index.min()\n",
    "    end_index = y.index.max()\n",
    "    # Define the p and q parameters to take any value between 0 and p_r/q_r, respectively.\n",
    "    p = range(0, pr)\n",
    "    q = range(0, qr)\n",
    "    \n",
    "    # Generate all different combinations of p and q\n",
    "    pq = list(itertools.product(p,q))\n",
    "    \n",
    "    columns = ['ARMA','r2_score','mae','mse','msle','rmse']\n",
    "    rows = []\n",
    "    for k in pq:\n",
    "        try:\n",
    "            model=sm.tsa.ARMA(train,order=k)\n",
    "            results = model.fit(method='css-mle')\n",
    "                \n",
    "            pred = results.predict(start=start_index, end=end_index, dynamic=True)    \n",
    "            \n",
    "            #mean_absolute_percentage_error = np.mean(np.abs((y - pred) / y)) * 100\n",
    "            #,mean_absolute_percentage_error)\n",
    "            #,'mape'\n",
    "            \n",
    "            row=[k,r2_score(y, pred),mean_absolute_error(y, pred),mean_squared_error(y, pred), mean_squared_log_error(y, pred), np.sqrt(mean_squared_error(y, pred))]\n",
    "            rows.append(row)\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot comparing prediction and actual values\n",
    "#Input: (dfDtransf.loc['data(starting point)':], Train dataframe, AR order, MA order)\n",
    "#In this case, the AR and MA order in the input is the actual order.\n",
    "def cross_validation_plot(y, train, test, p, q):\n",
    "    warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "\n",
    "    start_index = test.index.min()\n",
    "    end_index = test.index.max()\n",
    "    \n",
    "    model=sm.tsa.ARMA(train,order=(p,q))\n",
    "    results = model.fit(method='css-mle')\n",
    "                \n",
    "    pred = results.predict(start=start_index, end=end_index, dynamic=True) \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    y.plot(ax=ax)\n",
    "    pred.plot(ax=ax);\n",
    "    ax.legend([\"Observed\", \"Forecast\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Initial Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total number of 1-hour PM_10 Particles. It also counts the NA.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of missing PM_10 values in the whole dataframe.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second attribute = 1, since there are NA values on the dataframe.\n",
    "dataframe_analysis(df,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Resampling the Initial Data\n",
    "\n",
    "- As it was asked and in order to have an easier understanding of the data, it was created a **24-hour average** dataframe of the 1-hour $PM_{10}$ particle set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the desired 24-hour average dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df24 = df.resample('D').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General information** about the initial dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total number of 1-hour PM_10 Particles. It also counts the NA.\n",
    "df24.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of missing PM_10 values in the whole dataframe.\n",
    "df24.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second attribute = 1, since there are NA values on the dataframe.\n",
    "dataframe_analysis(df24,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Visual Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a new dataframe with only the **NAs objects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNA = df24.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNA.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an **array** with the values of $PM_{10}$ particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10 = dfNA['PM_10'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the NAs values with '1' and the others values with '0' in order analyze where the NAs are placed in the 5 year timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.nditer(pm10,op_flags=['readwrite']):\n",
    "    if np.isnan(i):\n",
    "        i[...]=1\n",
    "    else:\n",
    "        i[...]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creating a dataframe with the binary values of pm10\n",
    "\n",
    "- Replace the values of the column PM_10 from the dfNA dataframe with the values available in the pm10a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10a = pd.DataFrame(pm10)\n",
    "\n",
    "dfNA = dfNA.assign(PM_10=pm10a.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new column on dfNA dataframe with **trimester intervals** from *2014 till the end of 2018*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNA['Interval [Trimester]'] = pd.cut(dfNA.index,20,\n",
    "                        labels=['(2014-01-01,2014-04-02)','(2014-04-03,2014-07-02)',\n",
    "                                '(2014-07-03,2014-10-01)','(2014-10-02,2015-01-01)',\n",
    "                                '(2015-01-02,2015-04-02)','(2015-04-03,2015-07-02)',\n",
    "                                '(2015-07-03,2015-10-01)','(2015-10-02,2016-01-01)',\n",
    "                                '(2016-01-02,2016-04-01)','(2016-04-02,2016-07-01)',\n",
    "                                '(2016-07-02,2016-09-30)','(2016-10-01,2017-12-31)',\n",
    "                                '(2017-01-01,2017-04-01)','(2017-04-02,2017-07-01)',\n",
    "                                '(2017-07-02,2017-09-30)','(2017-10-01,2018-12-31)',\n",
    "                                '(2018-01-01,2018-04-01)','(2018-04-02,2018-07-01)',\n",
    "                                '(2018-07-02,2018-09-30)','(2018-10-01,2018-12-31)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNA['Month'] = pd.cut(dfNA.index, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe of dfNA where the values of $PM_{10}$ particles are grouped up by it's correspondent trimester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfNA = dfNA[['Interval [Trimester]','PM_10']].groupby('Interval [Trimester]').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfNA1 = dfNA[['Month','PM_10']].groupby(dfNA.index.month).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfNA.rename(columns={'PM_10':'NA'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfNA1.rename(columns={'PM_10':'NA'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar plot of the dataframe new.dfNA on which are able to see the number of days with N/A values in each trimester. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfNA.plot(figsize=(8, 6),kind='barh')\n",
    "plt.title('Number of days with NA values',**tfont)\n",
    "plt.ylabel('Trimester', **yfont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dfNA1.plot(figsize=(10, 6),kind='barh')\n",
    "plt.title('Number of days with NA values',**tfont)\n",
    "plt.ylabel('Month', **yfont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monthly** values of particles in order to see if there is any correlation between the time when there is a NA value and the actual PM_10 particle value at the same time. (time in month)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthly_correlation(df1,df2):\n",
    "    import plotly.io as pio\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    pio.templates.default = \"none\"\n",
    "    \n",
    "    dfM = df1.resample('M').mean()\n",
    "    \n",
    "    dfM['Month'] = dfM.index\n",
    "    \n",
    "    new_dfM = dfM[['Month','PM_10']].groupby(dfM.index.month).mean()\n",
    "    \n",
    "    new_dfM['Month NA'] = df2\n",
    "    \n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "          'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "    # Create figure with secondary y-axis\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # Add traces\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=months, y=new_dfM['PM_10'], name='Average PM_10 particles per month'),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=months, y=new_dfM['Month NA'], name=\"Number of Missig Values per month\"),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "    \n",
    "    # Add figure title\n",
    "    fig.update_layout(\n",
    "        font=dict(\n",
    "            family=\"sans-serif\",\n",
    "            size=12,\n",
    "            color=\"black\"\n",
    "        ),\n",
    "        legend=dict(\n",
    "            x=0,\n",
    "            y=1,\n",
    "            traceorder=\"normal\",\n",
    "            font=dict(\n",
    "                family=\"sans-serif\",\n",
    "                size=12,\n",
    "                color=\"black\"\n",
    "            )#,\n",
    "            #bgcolor=\"LightSteelBlue\",\n",
    "            #bordercolor=\"Black\",\n",
    "            #borderwidth=1\n",
    "        ),\n",
    "        barmode='group',\n",
    "        bargap=0.4, # gap between bars of adjacent location coordinates.\n",
    "        bargroupgap=0.2 # gap between bars of the same location coordinate.\n",
    "    )\n",
    "\n",
    "    # Set x-axis title\n",
    "    fig.update_xaxes(title_text=\"<b>Month<b>\")\n",
    "\n",
    "    # Set y-axes titles\n",
    "    fig.update_yaxes(title_text=r\"$PM_{10}$\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"<b>NAs<b>\", secondary_y=True, range=[0,12])\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "    dfM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_correlation(df,new_dfNA1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.1 Choosing the imputation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs:\n",
    "        #1 - 24h dataframe\n",
    "        #2 - n_rep - number of cycles\n",
    "        #3 - n_na - number of missing values on the new test dataframe\n",
    "        #4 - n_k - number of k neighbours\n",
    "imputation(df24,21, 2, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2.2 Applying the chosen imputation method - Linear Interpolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df24['rownum'] = np.arange(df24.shape[0])\n",
    "\n",
    "df24_nona = df24.dropna(subset = ['PM_10'])\n",
    "\n",
    "f = interp1d(df24_nona['rownum'], df24_nona['PM_10'])\n",
    "\n",
    "df24['PM_10_F'] = f(df24['rownum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD = df24.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD.drop(['PM_10','rownum'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD.rename(columns={'PM_10_F':'PM_10'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Dataframe Split in two parts\n",
    "\n",
    "- dfD_train contains all the observations till 26-12-2018;\n",
    "- dfD_test constains the last 5 observations (26-12-2018 till 31-12-2018);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD_train=dfD[:'2018-12-26'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD_test=dfD['2018-12-27':].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Statistical/Empirical Summary\n",
    "\n",
    "- Descriptive statistics\n",
    "- Skewness\n",
    "- Kurtosis\n",
    "- Histogram\n",
    "- QQ-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_analysis(dfD_train['PM_10'],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Time Series,  ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD_train.plot(figsize=(12, 4))\n",
    "pyplot.xlabel(\"Date\",**yfont)\n",
    "pyplot.ylabel(\"$PM_{10}$\", **yfont)\n",
    "pyplot.legend(\"\")\n",
    "pyplot.title(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(12,4))\n",
    "plot_acf(dfD_train,ax=pyplot.gca(), lags=40)\n",
    "pyplot.xlabel(\"Lag\",**yfont)\n",
    "pyplot.ylabel(\"ACF\", **yfont)\n",
    "pyplot.title(\"\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(12,4))\n",
    "plot_pacf(dfD_train,ax=pyplot.gca(), lags=40)\n",
    "pyplot.xlabel(\"Lag\",**yfont)\n",
    "pyplot.ylabel(\"PACF\", **yfont)\n",
    "pyplot.title(\"\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Decomposing\n",
    "\n",
    "- STL function: https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.STL.html\n",
    "- STL.fit function: https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.STL.fit.html#statsmodels.tsa.seasonal.STL.fit\n",
    "\n",
    "- seasonal_decompose function: https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (25, 15)\n",
    "\n",
    "plt.figure()\n",
    "decomp=STL(dfD_train)\n",
    "res = decomp.fit()\n",
    "resp=res.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (25, 15)\n",
    "decomp=seasonal_decompose(dfD_train,model='additive')\n",
    "decomp.plot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Differencing\n",
    "\n",
    "- It will be tested if there is a need for ARIMA differencing and SEASONAL differencing by using the pmdarima library.\n",
    "\n",
    "- ARIMA differencing function: https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.ndiffs.html\n",
    "\n",
    "- SEASONAL differencing function: https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.nsdiffs.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diffs = pm.ndiffs(dfD_train, max_d=10)\n",
    "print(\"n_diffs:\", n_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nsdiffs = pm.nsdiffs(dfD_train,5, max_D=10)\n",
    "print(\"n_nsdiffs:\", n_nsdiffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.9 Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.1 Boxcox Function\n",
    "\n",
    "- Box-cox transformation function: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html\n",
    "\n",
    "- lambda = -1. is a reciprocal transform;\n",
    "- lambda = -0.5 is a reciprocal square root transform;\n",
    "- lambda = 0.0 is a log transform;\n",
    "- lambda = 0.5 is a square root transform;\n",
    "- lambda = 1.0 is no transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD_trainT = dfD_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = boxcox(dfD_trainT['PM_10'].values)\n",
    "print('array:',a)\n",
    "#best lmbda = 0.02070343099348869"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD_trainT['PM_10'] = boxcox(dfD_trainT['PM_10'], lmbda=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming both dfD and the test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD_T = dfD.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD_T['PM_10'] = boxcox(dfD['PM_10'], lmbda=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD_testT = dfD_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD_testT['PM_10'] = boxcox(dfD_test['PM_10'], lmbda=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.2 KDE and QQ plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD_trainT['PM_10'].plot.kde(figsize=(12, 4))\n",
    "plt.xlim((1.7,5.4))\n",
    "#plt.xlim((-5,105))\n",
    "#plt.xlim((1.5,4.5))\n",
    "\n",
    "pyplot.xlabel(\"PM_10\",**yfont)\n",
    "pyplot.ylabel(\"Density\", **yfont)\n",
    "pyplot.legend(\"\")\n",
    "pyplot.title(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, axes = plt.subplots(nrows=1, ncols=1)\n",
    "qqplot(dfD_trainT['PM_10'], line='q', fit=True)\n",
    "pyplot.xlabel(\"Theoretical Quantiles\",**yfont)\n",
    "pyplot.ylabel(\"Sample Quantiles\", **yfont)\n",
    "pyplot.legend(\"\")\n",
    "pyplot.title(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe_analysis(dfD_trainT['PM_10'],0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.3 Time Series, ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfD_trainT.plot(figsize=(12, 4))\n",
    "pyplot.xlabel(\"Date\",**yfont)\n",
    "pyplot.ylabel(\"$log(PM_{10})$\", **yfont)\n",
    "pyplot.legend(\"\")\n",
    "pyplot.title(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(12,4))\n",
    "plot_acf(dfD_trainT,ax=pyplot.gca(), lags=40)\n",
    "pyplot.xlabel(\"Lag\",**yfont)\n",
    "pyplot.ylabel(\"ACF\", **yfont)\n",
    "pyplot.title(\"\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(12,4))\n",
    "plot_pacf(dfD_trainT,ax=pyplot.gca(), lags=40)\n",
    "pyplot.xlabel(\"Lag\",**yfont)\n",
    "pyplot.ylabel(\"PACF\", **yfont)\n",
    "pyplot.title(\"\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9.4 Decomposing\n",
    "\n",
    "- STL function: https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.STL.html\n",
    "- STL.fit function: https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.STL.fit.html#statsmodels.tsa.seasonal.STL.fit\n",
    "\n",
    "- seasonal_decompose function: https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (25, 15)\n",
    "\n",
    "plt.figure()\n",
    "decomp=STL(dfD_trainT).fit()\n",
    "resp=decomp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (25, 15)\n",
    "decomp=seasonal_decompose(dfD_trainT,model='additive')\n",
    "decomp.plot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Fitting and Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Model Fitting\n",
    "\n",
    "- arma function: https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARMA.html\n",
    "- auto_arima function: https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs:\n",
    "        #y - Dataframe\n",
    "        #p_r - max range of p\n",
    "        #q_r - max range of q\n",
    "ARMA_search(dfD_trainT,5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modl = auto_arima(dfD_trainT, start_p=1, start_q=1, max_p=10, max_q=10, seasonal=False, d=0, D=0, \n",
    "                     stepwise=True, suppress_warnings=True, error_action='ignore')\n",
    "\n",
    "print(\"model:\", modl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Residual Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Residuals Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.tsa.ARMA(dfD_trainT, order=(1,1))\n",
    "results = model.fit(method='css-mle')\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aicc = pm.ARIMA(order=(1, 0, 1)).fit(dfD_trainT).aicc()\n",
    "print('AICc:',aicc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_analysis(results.resid,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Residuals Time Series, KDE, QQ, ACF and PACF plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.resid.plot(figsize=(12, 4))\n",
    "pyplot.xlabel(\"Date\",**yfont)\n",
    "pyplot.ylabel(\"$log(PM_{10}).resid$\", **yfont)\n",
    "pyplot.legend(\"\")\n",
    "pyplot.title(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.resid.plot.kde(figsize=(12, 4))\n",
    "pyplot.xlabel(\"PM_10\",**yfont)\n",
    "pyplot.ylabel(\"Density\", **yfont)\n",
    "pyplot.legend(\"\")\n",
    "pyplot.title(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqplot(results.resid, line='q', fit=True, loc=0, scale=1)\n",
    "pyplot.xlabel(\"Theoretical Quantiles\",**yfont)\n",
    "pyplot.ylabel(\"Sample Quantiles\", **yfont)\n",
    "pyplot.legend(\"\")\n",
    "pyplot.title(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(12,4))\n",
    "plot_acf(results.resid,ax=pyplot.gca(), lags=20)\n",
    "pyplot.xlabel(\"Lag\",**yfont)\n",
    "pyplot.ylabel(\"ACF\", **yfont)\n",
    "pyplot.title(\"\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(12,4))\n",
    "plot_pacf(results.resid,ax=pyplot.gca(), lags=20)\n",
    "pyplot.xlabel(\"Lag\",**yfont)\n",
    "pyplot.ylabel(\"PACF\", **yfont)\n",
    "pyplot.title(\"\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Tests of Normality and Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Shapiro Wilk (Test of Normality)\n",
    "\n",
    "- Function: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html\n",
    "\n",
    "- **Output**: Shapiro-Wilk value; p-value;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.shapiro(results.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Jarque Bera Test (Test of Normality)\n",
    "\n",
    "- Function: https://www.statsmodels.org/devel/generated/statsmodels.stats.stattools.jarque_bera.html\n",
    "\n",
    "- **Output**: Jarque-Bera; p-value; estimated skewness; estimated kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.jarque_bera(results.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 Ljung-Cox Test (Test of Normality)\n",
    "- Function: https://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html\n",
    "\n",
    "- **Output**: Ljung-Box value; p-value;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.stats.acorr_ljungbox(results.resid,return_df=True, lags=10,model_df=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: (Test dataframe, Train dataframe, max AR order, max MA order)\n",
    "# It starts from 0, then if you want a max order of 3 for AR and MA, you need an input of 4 in both parameters.\n",
    "cross_validation_results(dfD_testT, dfD_trainT, 4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot comparing prediction and actual values\n",
    "\n",
    "#Input: (dfDtransf.loc['data(starting point)':], Train dataframe, Test dataframe, AR order, MA order)\n",
    "#In this case, the AR and MA order in the input is the actual order.\n",
    "cross_validation_plot(dfD_T.loc['2018-12-27':],dfD_trainT, dfD_testT,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input: (dataframe, order)\n",
    "model = sm.tsa.ARMA(dfD_T, order=(1,1))\n",
    "results = model.fit(method='css-mle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,6))\n",
    "fig=results.plot_predict(start='2018-12-23', end='2019-01-05', ax=ax)\n",
    "legend = ax.legend([\"Observed\", \"Forecast\",\"95% Conf. Interval\"], loc='upper left')\n",
    "ax.set_xlabel(\"Date\",**yfont)\n",
    "ax.set_ylabel(\"$log(PM_{10})$\", **yfont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = results.predict(start='2019-01-01', end='2019-01-05', dynamic=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrays of the function below:\n",
    "\n",
    "- forecastndarray\n",
    "    - Array of out of sample forecasts\n",
    "\n",
    "- stderrndarray\n",
    "    - Array of the standard error of the forecasts.\n",
    "\n",
    "- conf_intndarray\n",
    "    - 2d array of the confidence interval for the forecast\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.forecast(steps=5, alpha=.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
